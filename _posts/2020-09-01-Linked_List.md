---
layout: post
title: '[TH:DL] Simplest Encoder Decoder Model'
published: true
---

This post is going to present the simplest Encoder-Decoder model which reproduces a set of strings. Additionally, this post will also present the purpose of each layer used in Encoder and Decoder.
<p align="center">
<img src="/assets/2020-08-31-simple_encoder_decoder/seq2seq_model.png" alt="Simplest encoder-decoder architecture." width="800" >
</p>


Keywords:

1. Deep Learning
2. Encoder/Decoder
3. Embedding Layer
4. NLP

View the corresponding [Notebook](/assets/2020-08-31-simple_encoder_decoder/Simplest_Encoder_Decoder.ipynb).

<!--more-->

---

## 1. Sequence-to-Sequence model

Enter text in [Markdown](http://daringfireball.net/projects/markdown/). Use the toolbar above, or click the **?** button for formatting help.

---

## 2. Encoder

Enter text in [Markdown](http://daringfireball.net/projects/markdown/). Use the toolbar above, or click the **?** button for formatting help.

---

## 3. Decoder

Enter text in [Markdown](http://daringfireball.net/projects/markdown/). Use the toolbar above, or click the **?** button for formatting help.

---

## 4. Layer Purpose Explanation

Enter text in [Markdown](http://daringfireball.net/projects/markdown/). Use the toolbar above, or click the **?** button for formatting help.

---

## Further Learning

---

## References

1. [Deploying a seq2seq model with torchscript](https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html)

2. [The Annotated Encoder-Decoder with Attention](https://bastings.github.io/annotated_encoder_decoder/)

